---
---


@inproceedings{li2024challenging,
  abbr={ACL},
  bibtex_show={true},
  title={Challenging large language models with new tasks: A study on their adaptability and robustness},
  author={Li, Chenxi and Tian, Yuanhe and Zerong, Zhaxi and Song, Yan and Xia, Fei},
  booktitle={Findings of the Association for Computational Linguistics ACL 2024},
  pages={8140--8162},
  year={2024},
  pdf={/Users/wally/Desktop/UW/Project/wafflly.github.io/assets/pdf/Challenge_LLMs.pdf},
  abstract={Recent progress in large language models (LLMs) has marked a notable milestone in the field of artificial intelligence. The conventional evaluation of LLMs primarily relies on existing tasks and benchmarks, raising concerns about test set contamination and the genuine comprehension abilities of LLMs. To address these concerns, we propose to evaluate LLMs by designing new tasks, automatically generating evaluation datasets for the tasks, and conducting detailed error analyses to scrutinize LLMs’ adaptability to new tasks, their sensitivity to prompt variations, and their error tendencies. We investigate the capacity of LLMs to adapt to new but simple tasks, especially when they diverge from the models’ pre-existing knowledge. Our methodology emphasizes the creation of straightforward tasks, facilitating a precise error analysis to uncover the underlying causes of LLM failures. This strategic approach also aims to uncover effective strategies for enhancing LLM performance based on the detailed error analysis of system output.}
}


@inproceedings{zerong-etal-2025-systematic,
    abbr={EMNLP},
    bibtex_show={true},
    title = "A Systematic Survey of Claim Verification: Corpora, Systems, and Case Studies",
    pdf={/Users/wally/Desktop/UW/Project/wafflly.github.io/assets/pdf/CV-Survey.pdf},
    author = "Zerong, Zhaxi  and
      Li, Chenxi  and
      Liu, Xinyi  and
      Chen, Ju-hui  and
      Xia, Fei",
    editor = "Christodoulopoulos, Christos  and
      Chakraborty, Tanmoy  and
      Rose, Carolyn  and
      Peng, Violet",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2025",
    month = nov,
    year = "2025",
    url = "https://aclanthology.org/2025.findings-emnlp.1170/",
    pages = "21452--21474",
    ISBN = "979-8-89176-335-7",
    abstract = "Automated Claim Verification (CV){---}the task of assessing a claim{'}s veracity against explicitly provided evidence{---}is a critical tool in the fight against growing misinformation. This survey offers a comprehensive analysis of 198 studies published between January 2022 and March 2025, synthesizing recent advances in CV corpus creation and system design. Through two in-depth case studies, we illuminate persistent challenges in veracity annotation, limitations of conventional CV pipelines, and pitfalls in recent claim decomposition approaches. We conclude by identifying key unresolved challenges and proposing productive directions for future research."
}


@inproceedings{zhang2025UW-BioNLP,
  abbr={Clinical NLP},
  bibtex_show={false},
  title={UW-BioNLP at ChemoTimelines 2025: Thinking, Fine-Tuning, and Dictionary-Enhanced LLM Systems for Chemotherapy Timeline Extraction},
  author={Zhang, Tianmai M and Sun, Zhaoyi and Zeng, Sihang and Li, Chenxi and Abernethy, Neil F and Lam, Barbara D and Xia, Fei and Yetisgen, Meliha},
  booktitle={The 7th Clinical Natural Language Processing Workshop},
  month = oct,
  year={2025},
  pdf={/Users/wally/Desktop/UW/Project/wafflly.github.io/assets/pdf/Chemo2025.pdf},
  note={Pending publictation},
  abstract={The ChemoTimelines shared task benchmarks methods for constructing timelines of systemic anticancer treatment from electronic health records of cancer patients. This paper describes our methods, results, and findings for subtask2—generating patient chemotherapy timelines from raw clinical notes. We evaluated strategies involving chain-of-thought thinking, supervised fine-tuning, direct preference optimization, and dictionary-based lookup to improve timeline extraction. All of our approaches followed a two-step workflow, wherein an LLM first extracted chemotherapy events from individual clinical notes, and then an algorithm normalized and aggregated events into patient-level timelines. Each specific method differed in how the associated LLM was utilized and trained. Multiple approaches yielded competitive performances on the test set leaderboard, with fine-tuned Qwen3-14B achieving the best official score of 0.678. Our results and analyses could provide useful insights for future attempts on this task as well as the design of similar tasks.}
}